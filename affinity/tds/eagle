#!/usr/bin/bash
#SBATCH --job-name="start_vasp"
#SBATCH --nodes=2
#SBATCH --exclusive
#SBATCH --export=ALL
#SBATCH --time=01:00:00
#SBATCH --partition=debug
##SBATCH --tasks-per-node=$ntpn $CPUS_TASK
###########SBATCH --account=hpcapps

BASE=/sfs/projects/hpcapps/examples/all
BASE=`pwd`

#Make a new directory
STDIR=`pwd`
mkdir $SLURM_JOB_ID
cd $SLURM_JOB_ID

#Copy everything 
printenv > env
cat $0 > script
cp $BASE/make* .
cp $BASE/Makefile .
cp $BASE/fhostone.F90 .
cp $BASE/phostone.c .
if [ "$SLURM_CLUSTER_NAME" = "swift" ]; then cp $BASE/scases cases ; fi
if [ "$SLURM_CLUSTER_NAME" = "eagle" ]; then cp $BASE/ecases cases ; fi
cp $BASE/post .
cp $BASE/ppong.c .
cp $BASE/getcore.c .
cp $BASE/maskgenerator.py .

#Build our programs
make -f make1api
make -f makeopen

export OMP_NUM_THREADS=2
CLA="-F -E -t 10"

export BIND=v,cores
#export CRAY_OMP_CHECK_AFFINITY=TRUE
#export OMP_PROC_BIND=spread
export nc=`cat cases | wc -l`
#for bindit in none rank rank_ldom threads ldoms ; do
for il in `seq $nc` ; do
  aline=`cat cases | head -$il | tail -1`
  ntpn=`echo $aline | awk {'print $1'}`
  nthrd=`echo $aline | awk {'print $2'}`
  export OMP_NUM_THREADS=$nthrd
#for bindit in MASK ; do
for bindit in NONE MASK ; do
  #export KMP_AFFINITY=scatter
  export OMP_PLACES=cores
  export OMP_PROC_BIND=spread
  export BIND=--cpu-bind=v,${bindit}
  unset CPUS_TASK
  #export CPUS_TASK="--cpus-per-task=$nthrd"
  if [ $bindit == MASK ] ; then
	  cores=`expr $ntpn \* $nthrd`
	  MASK=`./maskgenerator.py $cores $ntpn`
	  BIND="--cpu-bind=v,mask_cpu:$MASK"
	  
  fi
  if [ $bindit == NONE ] ; then
	  BIND="--cpu-bind=v"
          export CPUS_TASK="--cpus-per-task=$nthrd"
  fi
  echo $ntpn $nthrd >> srunsettings
  printenv | egrep "OMP_|KMP_" >> srunsettings
  echo --threads-per-core=1 --mpi=pmi2 $BIND  --tasks-per-node=$ntpn $CPUS_TASK >> srunsettings
#Run each version

module purge
ml slurm/21-08-1-1-o2xw5ti  2>/dev/null || :
ml intel-oneapi-mpi
ml intel-oneapi-compilers
#NOTE: We don't load intel-oneapi-mpi
#If you get runtime like errors talk to tkaiser2
#module load intel-oneapi-mpi
srun --threads-per-core=1 --mpi=pmi2 $BIND  --tasks-per-node=$ntpn $CPUS_TASK  ./f.impi $CLA > f.impi.out_${ntpn}_${nthrd}_${bindit} 2> f.impi.err_${ntpn}_${nthrd}_${bindit}
srun --threads-per-core=1 --mpi=pmi2 $BIND  --tasks-per-node=$ntpn $CPUS_TASK  ./c.impi $CLA > c.impi.out_${ntpn}_${nthrd}_${bindit} 2> c.impi.err_${ntpn}_${nthrd}_${bindit}


module purge 
ml slurm/21-08-1-1-o2xw5ti 2>/dev/null || :
module load openmpi/4.1.2/intel || module load openmpi/4.1.1-6vr2flz
module load gcc
srun --threads-per-core=1 --mpi=pmi2 $BIND  --tasks-per-node=$ntpn $CPUS_TASK  ./f.open $CLA > f.open.out_${ntpn}_${nthrd}_${bindit} 2> f.open.err_${ntpn}_${nthrd}_${bindit}
srun --threads-per-core=1 --mpi=pmi2 $BIND  --tasks-per-node=$ntpn $CPUS_TASK  ./c.open $CLA > c.open.out_${ntpn}_${nthrd}_${bindit} 2> c.open.err_${ntpn}_${nthrd}_${bindit}

done
done

. ./post | sort -n
mv $STDIR/slurm-$SLURM_JOB_ID.out .
